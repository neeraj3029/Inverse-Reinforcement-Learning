{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1XdtBLFIMeZDO6_cl0_2pspYEISWG-9R_",
      "authorship_tag": "ABX9TyPbjYIXDgIirCH1Lo0GRdU4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divypandya/Inverse-Reinforcement-Learning/blob/divypandya-patch-1/IRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpYMXL3Dk0mc",
        "colab_type": "text"
      },
      "source": [
        "# **Linear Programming formulation** \n",
        "#### **Inverse Reinforcement Learning in Ng & Russell 2000 paper: Algorithms for Inverse Reinforcement Learning**\n",
        "##### [Paper Link](https://ai.stanford.edu/~ang/papers/icml00-irl.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFiZog4Dl09u",
        "colab_type": "text"
      },
      "source": [
        "*   **Here we have implemented IRL in Finite State Space using Linear Programming Optization** \n",
        "\n",
        "*   **Given**: \n",
        " * A finite state space $S$ of $N$ **states**. \n",
        " * $A = \\{a_1,...,a_k\\}$ is set of $k$ actions.\n",
        " * $P_{sa}(.)$ are the state **transition probabilities** upon taking action $a$ in state $s$.\n",
        " * $\\gamma \\in [0, 1)$ is the **discount factor**.\n",
        " * $R \\ \\ : S \\rightarrow {\\rm I\\!R}$ is the **reinforcement function**, bounded in absolute way by $R_{max}$\n",
        "\n",
        "*  **Goal** : We wish to find the set of possible reward functions $R$ such that $\\pi$ is an optimal policy in MDP $(S, A, \\{P_{sa}\\}, \\gamma, R)$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8KnXxKY0zjt",
        "colab_type": "text"
      },
      "source": [
        "#### **Optimization Problem**\n",
        "\n",
        "Given optimal policy $\\pi(s)\\equiv a_1$ our aim is to:\n",
        "\n",
        ">  $ max \\ \\ \\sum^{N}_{i=1} min_{a \\in \\{a_2,...,a_k\\}} \\{\\ (P_{a_1}(i) - P_a(i))\\ (I - \\gamma P_{a_1})^{-1}R\\} - \\lambda ||R||_1$\n",
        "\n",
        "> $ \\ \\ s.t. \\ \\ (P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1}R \\ \\succeq 0, \\ \\ \\forall a \\in A\\ \\backslash \\ a_1 $ \\\\\n",
        "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  |R_i| \\leq R_{max}, \\ \\ i = 1,...,N$\n",
        "\n",
        "where $P_a$ is $N \\times N$ transition matrix for action $a$ and $P_a(i)$ denotes the $i^{th}$ row of $P_a$.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "##### Above optimization problem can be written as a set of matrix equations.\n",
        "\n",
        "> $max \\ \\ \\{ \\sum^{N}_{i=1}\\{m_i - \\lambda u_i\\}\\}$ <br>\n",
        "> $\\ \\ s.t. \\ \\ \\  m_i = min_{a \\in A\\backslash a_1} \\{(P_{a_1}(i) - P_{a}(i))(I - \\gamma P_a)^{-1}R\\}, \\ \\ i = 1,...,N $ <br>\n",
        "> $ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (P_{a_1} - P_a)(I - \\gamma P_{a_1}) \\succeq 0, \\ \\ \\ \\forall a \\in A\\backslash a_1, \\ \\ i = 1,..., N $ <br>\n",
        "> $ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ -U \\leq R \\leq U$ <br>\n",
        "> $ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ |R_i| \\leq R_{max}$\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "##### Now let's vectorize the above optimization task\n",
        "\n",
        "* First select two dummy vectors $M$ and $U$ of length $N$,\n",
        "\n",
        "* Construct inequality constraints matrix $A$. Each row of $A$ specifies coefficients of a linear inequality. Size of $A$ is $[2N(K+1), 3N]$\n",
        "\n",
        "* $A = $\n",
        "\n",
        "   \\begin{bmatrix}\n",
        "    -(P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} & I & 0 \\\\\n",
        "    -(P_{a_1} - P_a)(I - \\gamma P_{a_1})^{-1} & 0 & 0 \\\\\n",
        "    I & 0 & 0 \\\\\n",
        "    -I & 0 & 0 \\\\\n",
        "    I & 0 & -I \\\\\n",
        "    -I & 0 & -I\n",
        "   \\end{bmatrix}\n",
        "<br>\n",
        "*  A vector $c$ of length $3N$ which contains cofficiants of linear objective function to be minimized, i.e. <br>\n",
        "$c = $\n",
        "\n",
        "\\begin{bmatrix}\n",
        "    0\\\\\n",
        "    1\\\\\n",
        "    -\\lambda 1\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "*  Also a vector $x$, which is to be found, of length $3N$ as <br>\n",
        "\n",
        "\\begin{bmatrix}\n",
        "    R\\\\\n",
        "    M\\\\\n",
        "    U\n",
        "\\end{bmatrix} \n",
        "\n",
        "<br>\n",
        "\n",
        "* vector $b$, the inequality contraint vector. Each element represents an upper bound on the corresponding value of $Ax$. \\\\\n",
        "Size of b = $[2NK + 2N]$ <br>\n",
        "\n",
        " $b = $\n",
        "\n",
        "\\begin{bmatrix}\n",
        "    0\\\\\n",
        "    0\\\\\n",
        "    R_{max}\\\\\n",
        "    R_{max}\\\\\n",
        "    0\\\\\n",
        "    0\n",
        "\\end{bmatrix} \n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "##### Therefore now our new modified well vectorized optimized task is as follows: \n",
        "\n",
        "\\begin{equation}\n",
        "    argmax_x \\ \\ c^{T}x \\\\ \n",
        "    \\ \\ s.t. \\ \\ \\  Ax \\leq b\n",
        "\\end{equation}\n",
        "\n",
        "Below is the python code for same.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cMbfDHS8Or5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy.optimize as opt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcV9tkupr0Tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(val):\n",
        "    min_val = np.min(val)\n",
        "    max_val = np.max(val)\n",
        "\n",
        "    return (val - min_val) / (max_val - min_val)\n",
        "\n",
        "def linear_prog_irl(trans_probs, policy, gamma=0.1, l1=0.5, R_max=10):\n",
        "    '''\n",
        "    inputs:\n",
        "        trans_probs       NxNx(N_ACTIONS) transition matrix, where N is #states\n",
        "        policy            policy vector / map\n",
        "        R_max             maximum possible value of recoverred rewards\n",
        "        gamma             RL discount factor\n",
        "        l1                l1 regularization parameter lambda\n",
        "\n",
        "    returns:\n",
        "        rewards           Nx1 reward vector\n",
        "\n",
        "    '''\n",
        "    \n",
        "    N_s, _, N_a = np.shape(trans_probs)\n",
        "    N_s = int(N_s)\n",
        "    N_a = int(N_a)\n",
        "\n",
        "    # Formulate a linear IRL problem\n",
        "    A = np.zeros([2 * N_s * (N_a + 1), 3 * N_s])\n",
        "    b = np.zeros([2 * N_s * (N_a + 1)])\n",
        "    c = np.zeros([3 * N_s])\n",
        "\n",
        "    for i in range(N_s):\n",
        "        a_opt = int(policy[i])\n",
        "        tmp_inv = np.linalg.inv(np.identity(N_s) - gamma * trans_probs[:, :, a_opt])\n",
        "\n",
        "        cnt = 0\n",
        "        for a in range(N_a):\n",
        "            if a != a_opt:\n",
        "                A[i * (N_a - 1) + cnt, :N_s] = - \\\n",
        "                    np.dot(trans_probs[i, :, a_opt] - trans_probs[i, :, a], tmp_inv)\n",
        "                A[N_s * (N_a - 1) + i * (N_a - 1) + cnt, :N_s] = - \\\n",
        "                    np.dot(trans_probs[i, :, a_opt] - trans_probs[i, :, a], tmp_inv)\n",
        "                A[N_s * (N_a - 1) + i * (N_a - 1) + cnt, N_s + i] = 1\n",
        "                cnt += 1\n",
        "\n",
        "    for i in range(N_s):\n",
        "        A[2 * N_s * (N_a - 1) + i, i] = 1\n",
        "        b[2 * N_s * (N_a - 1) + i] = R_max\n",
        "\n",
        "    for i in range(N_s):\n",
        "        A[2 * N_s * (N_a - 1) + N_s + i, i] = -1\n",
        "        b[2 * N_s * (N_a - 1) + N_s + i] = 0\n",
        "\n",
        "    for i in range(N_s):\n",
        "        A[2 * N_s * (N_a - 1) + 2 * N_s + i, i] = 1\n",
        "        A[2 * N_s * (N_a - 1) + 2 * N_s + i, 2 * N_s + i] = -1\n",
        "\n",
        "    for i in range(N_s):\n",
        "        A[2 * N_s * (N_a - 1) + 3 * N_s + i, i] = 1\n",
        "        A[2 * N_s * (N_a - 1) + 3 * N_s + i, 2 * N_s + i] = -1\n",
        "\n",
        "    for i in range(N_s):\n",
        "        c[N_s:2 * N_s] = -1\n",
        "        c[2 * N_s:] = l1\n",
        "\n",
        "    res = opt.linprog(c, A_ub=A, b_ub=b)\n",
        "    reward = res.x[:N_s]\n",
        "    reward = normalize(reward) * R_max\n",
        "\n",
        "    return reward\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAXeCbptw0Pm",
        "colab_type": "text"
      },
      "source": [
        "# **Results**\n",
        "### Ground Truth Reward Function\n",
        "<div>\n",
        "<img src= \"https://drive.google.com/uc?id=1E2DrWHR4xAJZ2gSkSr6NPKMnfip1keuU\" width=\"300\" height=\"300\"/> &emsp; &emsp; &emsp; &emsp;\n",
        "<img src= \"https://drive.google.com/uc?id=1-HRmYDX2SaSvhvCmPbwF2C7XunUH_mdA\" width=\"300\" height='300'/>\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "### Recovered Reward Function\n",
        "<div>\n",
        "<img src= \"https://drive.google.com/uc?id=1-8dEOXwpzdxax5PkZS64hVyr4FWYhU3J\" width=\"300\" height=\"300\"/> &emsp; &emsp; &emsp; &emsp;\n",
        "<img src= \"https://drive.google.com/uc?id=1-9hjln_yqCV9QOcDIjyxbVsWwc9Kp_Q2\" width=\"300\" height='300'/>\n",
        "</div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWrB-qFt5hm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}